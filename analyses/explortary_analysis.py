# -*- coding: utf-8 -*-
"""scholawrite-eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IX2BzFzT6PaQxs57Z__M8t_0AOOOz6DD
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import polars as pls

"""# Data Loading"""

data1 = pd.read_parquet('/content/all_sorted-00000-of-00002.parquet', engine='pyarrow')
data2 = pd.read_parquet('/content/all_sorted-00001-of-00002.parquet', engine='pyarrow')

test_data1 = pd.read_parquet('/content/test-00000-of-00001.parquet', engine='pyarrow')
test_data2 = pd.read_parquet('/content/test_small-00000-of-00001.parquet', engine='pyarrow')
train_data2 = pd.read_parquet('/content/train-00000-of-00001.parquet', engine='pyarrow')

test_data1.shape, test_data2.shape, train_data2.shape

data1.head()

"""# Data Processing"""

train_data = pd.concat([data1, data2], ignore_index=True)
train_data.head()

train_data.shape, data1.shape, data2.shape

test_data = pd.concat([test_data1, test_data2], ignore_index=True)
train_data.head()

test_data.shape, train_data2.shape

data = pd.concat([train_data, test_data, train_data2], ignore_index=True)
data.head()

data.shape

unique_data = data.drop_duplicates()
unique_data.shape

data = train_data
data.shape

"""# Data Grouping"""

grouped = data.groupby('project')
project_tables = {project: group.copy() for project, group in grouped}
project_tables

project_tables[1].shape, project_tables[2].shape, project_tables[3].shape, project_tables[4].shape, project_tables[5].shape

for i in range(5):
    project_tables[i+1] = project_tables[i+1].sort_values(by='timestamp')
project_tables[1].head()

"""# Visualization Summaries

## Data Distributions
"""

def plot_label_and_highlevel_distribution(idx):
    df = project_tables[idx].copy()

    df = df.dropna(subset=['label', 'high-level'])

    label_counts = df['label'].value_counts()
    highlevel_counts = df['high-level'].value_counts()

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    axes[0].bar(label_counts.index, label_counts.values, color='skyblue')
    axes[0].set_title(f'Project {idx} - Label Distribution')
    axes[0].set_xlabel('Label')
    axes[0].set_ylabel('Count')
    axes[0].tick_params(axis='x', rotation=45)
    axes[1].bar(highlevel_counts.index, highlevel_counts.values, color='lightgreen')
    axes[1].set_title(f'Project {idx} - High-Level Distribution')
    axes[1].set_xlabel('High-Level Category')
    axes[1].set_ylabel('Count')
    axes[1].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

for i in range(1, 6):
    plot_label_and_highlevel_distribution(i)

"""## Hierachichal Distributions"""

!pip install circlify

import circlify
from collections import defaultdict

def get_hierarchy_data(df):
    counts = df.groupby(['high-level', 'label']).size().reset_index(name='value')
    hierarchy = []

    for hl in counts['high-level'].unique():
        children = []
        for _, row in counts[counts['high-level'] == hl].iterrows():
            children.append({
                "id": row["label"],
                "value": int(row["value"])
            })
        hierarchy.append({
            "id": hl,
            "children": children
        })

    return {"id": "root", "children": hierarchy}



def plot_circle_packing(df, project_id):
    # Step 1: 计算频数
    counts = df.groupby(['high-level', 'label']).size().reset_index(name='value')

    # Step 2: 构造 circlify 所需数据结构
    data = []
    for hl in counts['high-level'].unique():
        group = counts[counts['high-level'] == hl]
        children = []
        for _, row in group.iterrows():
            children.append({'id': row['label'], 'value': int(row['value'])})
        data.append({'id': hl, 'children': children})

    # Step 3: 运行 circlify
    circles = circlify.circlify(
        data,
        show_enclosure=True,
        target_enclosure=circlify.Circle(x=0, y=0, r=1),
        datum_field='value' # Specify that 'value' key holds the size
    )

    # Step 4: 绘图
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.axis('off')

    for circle in circles:
        x, y, r = circle.x, circle.y, circle.r
        label = circle.ex.get('id', '')
        # Check the level attribute to differentiate between high-level and label circles
        if circle.level == 2: # Level 2 usually corresponds to the inner circles (labels)
            ax.add_patch(plt.Circle((x, y), r, alpha=0.8, color='orange'))
            # Only add text for circles that are large enough to display it clearly
            if r > 0.05: # You might need to adjust this threshold
                plt.text(x, y, label, ha='center', va='center', fontsize=7)
        elif circle.level == 1: # Level 1 usually corresponds to the outer circles (high-level)
            ax.add_patch(plt.Circle((x, y), r, alpha=0.2, color='blue'))
            # Only add text for circles that are large enough to display it clearly
            if r > 0.1: # You might need to adjust this threshold
                plt.text(x, y, label, ha='center', va='center', fontsize=9)


    plt.title(f"Project {project_id}: Hierarchical Edit Structure")
    plt.tight_layout()
    plt.show()

for i in range(1, 6):
    df = project_tables[i].copy()
    df = df.dropna(subset=['label', 'high-level'])
    plot_circle_packing(df, i)

"""# Temporal Data Visual Exploration"""

# preparation
unique_labels = data['label'].dropna().unique().tolist()
unique_hl_labels = data['high-level'].dropna().unique().tolist()
unique_labels, unique_hl_labels

"""## Temporal Data Visualization Summaries(High-Level)"""

def draw_temporal_label_frequency(idx, column, labels):
    df = project_tables[idx].copy()

    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')
    df = df.dropna(subset=['timestamp'])

    freq_series_list = []
    valid_labels = []

    for label in labels:
        df_label = df[df[column] == label]
        if not df_label.empty:
            freq_series = df_label.resample('D', on='timestamp').size()
            freq_series_list.append(freq_series)
            valid_labels.append(label)

    if not freq_series_list:
        print(f"[Project {idx}] No valid data for any of the provided labels.")
        return

    plt.figure(figsize=(10, 5))
    for i in range(len(freq_series_list)):
        plt.plot(freq_series_list[i].index, freq_series_list[i].values, marker='o', linestyle='-', label=valid_labels[i])

    plt.title(f"Project {idx}: Daily Frequency of {column} Labels Over Time")
    plt.xlabel('Date')
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

for i in range(5):
    draw_temporal_label_frequency(i+1, "high-level", unique_hl_labels)

"""## Temporal Data Visualization Summaries(Detail-Level)"""

for i in range(5):
    draw_temporal_label_frequency(i+1, "label", unique_labels)

"""## Temporal Data Visualization Summaries(Author-level)"""

def draw_temporal_author_frequency(idx, authors):
    df = project_tables[idx].copy()

    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')
    df = df.dropna(subset=['timestamp'])

    freq_series_list = []
    valid_authors = []

    for author in authors:
        df_author = df[df['author'] == author]
        if not df_author.empty:
            freq_series = df_author.resample('D', on='timestamp').size()
            freq_series_list.append(freq_series)
            valid_authors.append(author)

    if not freq_series_list:
        print(f"[Project {idx}] No valid data for any of the provided authors.")
        return

    plt.figure(figsize=(10, 5))
    for i in range(len(freq_series_list)):
        plt.plot(freq_series_list[i].index, freq_series_list[i].values, marker='o', linestyle='-', label=str(valid_authors[i]))

    plt.title(f"Project {idx}: Daily Frequency of Authors Over Time")
    plt.xlabel('Date')
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

for i in range(5):
    # unique authors
    unique_authors = project_tables[i+1]['author'].dropna().unique().tolist()
    # visualization
    draw_temporal_author_frequency(i+1, unique_authors)

"""# Text Data Visual Exploration

## Text Difference Summaries

- edit_counts: (add_counts, del_counts), provides summary data for edit counting
- add_counts: add word count
- del_counts: delete word count
"""

from difflib import ndiff

def compute_edit_counts(before, after):
    diff = list(ndiff(before.split(), after.split()))
    added = sum(1 for w in diff if w.startswith('+ '))
    removed = sum(1 for w in diff if w.startswith('- '))
    return (added, removed)

for i in range(len(project_tables)):
    df = project_tables[i+1].copy()
    df['edit_counts'] = df.apply(lambda row: compute_edit_counts(row['before text'], row['after text']), axis=1)
    df[['add_count', 'del_count']] = pd.DataFrame(df['edit_counts'].tolist(), index=df.index)
    project_tables[i+1] = df

project_tables[1].head()

"""### Difference Types Distributions"""

for i in range(5):
    df = project_tables[i+1]

    plt.figure(figsize=(8, 6))
    plt.scatter(df['add_count'], df['del_count'], alpha=0.5)
    plt.xlabel('Tokens Added')
    plt.ylabel('Tokens Deleted')
    plt.title(f'Project {i+1}: Edit Distribution: Added vs Deleted Tokens')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""### Correlation between Intentions and Editing Counts"""

for i in range(5):
    df = project_tables[i+1].copy()
    df = df.dropna(subset=['label', 'add_count', 'del_count'])

    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    sns.boxplot(x='label', y='add_count', data=df)
    plt.title(f'Project {i+1}: Tokens Added by Label')
    plt.xticks(rotation=45)
    plt.ylabel('Tokens Added')

    plt.subplot(1, 2, 2)
    sns.boxplot(x='label', y='del_count', data=df)
    plt.title(f'Project {i+1}: Tokens Deleted by Label')
    plt.xticks(rotation=45)
    plt.ylabel('Tokens Deleted')

    plt.tight_layout()
    plt.show()

# One-way ANOVA
from scipy.stats import f_oneway

for i in range(1, 6):
    df = project_tables[i].copy()
    df = df.dropna(subset=['label', 'add_count', 'del_count'])

    groups_add = [group['add_count'].values for name, group in df.groupby('label')]
    groups_del = [group['del_count'].values for name, group in df.groupby('label')]

    f_add, p_add = f_oneway(*groups_add)
    f_del, p_del = f_oneway(*groups_del)

    print("Project " + str(i))
    print("ANOVA for add_count by label: F = {:.4f}, p = {:.4f}".format(f_add, p_add))
    print("ANOVA for del_count by label: F = {:.4f}, p = {:.4f}".format(f_del, p_del))

"""We conducted one-way ANOVA tests to examine whether different revision intentions (labels) significantly affect editing behaviors (token-level additions and deletions). Across all five projects, the results showed statistically significant effects of labels on both tokens added (all p < 0.001) and tokens deleted (all p < 0.001), with F-values ranging from 6.58 to 100.37. These findings suggest that revision intention strongly correlates with the scale and type of text changes made by authors."""

from statsmodels.stats.multicomp import pairwise_tukeyhsd

df = project_tables[1].dropna(subset=['label', 'add_count'])
tukey_add = pairwise_tukeyhsd(endog=df['add_count'], groups=df['label'], alpha=0.05)
print(tukey_add.summary())

results = pd.DataFrame(data=tukey_add._results_table.data[1:], columns=tukey_add._results_table.data[0])

labels = sorted(set(results['group1']).union(set(results['group2'])))
matrix = pd.DataFrame(np.nan, index=labels, columns=labels)

for _, row in results.iterrows():
    g1 = row['group1']
    g2 = row['group2']
    diff = row['meandiff'] if row['reject'] else np.nan
    matrix.loc[g1, g2] = diff
    matrix.loc[g2, g1] = -diff if row['reject'] else np.nan

plt.figure(figsize=(12, 10))
sns.heatmap(matrix, annot=True, fmt=".1f", cmap="RdBu_r", center=0, linewidths=0.5, cbar_kws={'label': 'Mean Add Count Difference'})
plt.title("Tukey HSD: Pairwise Differences in Add Count by Label (Significant Only)")
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

for i in range(1, 6):
    df = project_tables[i].copy()
    df = df.dropna(subset=['label', 'add_count', 'del_count'])

    mean_edits = df.groupby('label')[['add_count', 'del_count']].mean()

    sns.heatmap(mean_edits, annot=True, cmap='YlGnBu')
    plt.title(f"Project {i}: Mean Edits per Label")
    plt.show()

"""## Semantic Quantitive Analysis"""

def extract_added_deleted_words(before, after):
    before_tokens = str(before).split()
    after_tokens = str(after).split()
    diff = list(ndiff(before_tokens, after_tokens))
    add_words = [token[2:] for token in diff if token.startswith('+ ')]
    del_words = [token[2:] for token in diff if token.startswith('- ')]
    return add_words, del_words

for i in range(1, 6):  # Project 1 ~ 5
    df = project_tables[i].copy()
    df[['add_words', 'del_words']] = df.apply(
        lambda row: pd.Series(extract_added_deleted_words(row['before text'], row['after text'])),
        axis=1
    )
    project_tables[i] = df

project_tables[1].head()

"""### Word Clouds(word addings, words deleting)"""

from wordcloud import WordCloud
from collections import Counter

all_add_words = []
all_del_words = []

for i in range(1, 6):
    df = project_tables[i]

    add_words = sum(df['add_words'].dropna(), [])
    del_words = sum(df['del_words'].dropna(), [])

    all_add_words.extend(add_words)
    all_del_words.extend(del_words)

def plot_wordcloud(words, title, color='black'):
    text = ' '.join(words)
    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)

    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontsize=16)
    plt.axis('off')
    plt.show()

plot_wordcloud(all_add_words, "Word Cloud of Added Words")
plot_wordcloud(all_del_words, "Word Cloud of Deleted Words")

"""### POS Analysis"""

import spacy

nlp = spacy.load('en_core_web_sm')


all_add_words = []
all_del_words = []

for i in range(1, 6):
    df = project_tables[i]
    all_add_words.extend(sum(df['add_words'].dropna(), []))
    all_del_words.extend(sum(df['del_words'].dropna(), []))

def get_pos_counts(word_list, batch_size=50000):
    pos_counter = Counter()
    for i in range(0, len(word_list), batch_size):
        batch = word_list[i:i + batch_size]
        text = ' '.join(batch)
        doc = nlp(text)
        pos_counter.update(token.pos_ for token in doc)
    return pos_counter

add_pos = get_pos_counts(all_add_words)
del_pos = get_pos_counts(all_del_words)

def plot_pos_distribution(pos_counts, title):
    labels, values = zip(*pos_counts.items())
    plt.figure(figsize=(10, 5))
    plt.bar(labels, values, color='skyblue')
    plt.title(title)
    plt.xlabel("Part of Speech")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

plot_pos_distribution(add_pos, "POS Distribution of Added Words")
plot_pos_distribution(del_pos, "POS Distribution of Deleted Words")

"""### Semantic Categorical Analysis"""

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import wordnet as wn

def get_primary_hypernym(word):
    synsets = wn.synsets(word)
    if not synsets:
        return None
    hypernyms = synsets[0].hypernyms()
    if not hypernyms:
        return None
    return hypernyms[0].lemma_names()[0]

from collections import Counter

add_hypernyms = [get_primary_hypernym(w) for w in all_add_words]
del_hypernyms = [get_primary_hypernym(w) for w in all_del_words]

add_hypernyms = [h for h in add_hypernyms if h is not None]
del_hypernyms = [h for h in del_hypernyms if h is not None]

add_hypernym_counts = Counter(add_hypernyms).most_common(20)
del_hypernym_counts = Counter(del_hypernyms).most_common(20)

def plot_top_hypernyms(hypernym_counts, title):
    labels, values = zip(*hypernym_counts)
    plt.figure(figsize=(10, 5))
    plt.barh(labels[::-1], values[::-1], color='orange')
    plt.title(title)
    plt.xlabel('Frequency')
    plt.tight_layout()
    plt.show()

plot_top_hypernyms(add_hypernym_counts, "Top Semantic Categories of Added Words (WordNet Hypernyms)")
plot_top_hypernyms(del_hypernym_counts, "Top Semantic Categories of Deleted Words (WordNet Hypernyms)")

"""## Clusters Analysis"""



"""# Questions
- sample size, is five too small?
- few concerns about limitations
- only human keystroke dataset?
"""

