# -*- coding: utf-8 -*-
"""ScholaWrite-LLM-Outputs-Analyses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15NapUB1oOK3CvB-eNjrY0Pd3J6prTi8f
"""

import zipfile
import os

import os
import zipfile
import pandas as pd

def find_subdir_with(target_names, root_dir):
    for dirpath, dirnames, _ in os.walk(root_dir):
        for name in target_names:
            if name in dirnames:
                return os.path.join(dirpath, name)
    return None

def process_zip_to_csv(zip_path, output_folder, model_name):
    import tempfile
    import shutil

    temp_dir = tempfile.mkdtemp()
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # 搜索可能存在的 generation 和 intention 路径（支持任意嵌套）
    gen_dir = find_subdir_with(["generation_openai", "generation"], temp_dir)
    int_dir = find_subdir_with(["intention_openai", "intention"], temp_dir)

    if not gen_dir or not int_dir:
        raise FileNotFoundError("Cannot find generation or intention directory in the zip.")

    generation_files = sorted([
        os.path.join(gen_dir, f) for f in os.listdir(gen_dir)
        if f.startswith("iter_generation_") and f.endswith(".txt")
    ], key=lambda x: int(x.split('_')[-1].split('.')[0]))

    intention_files = sorted([
        os.path.join(int_dir, f) for f in os.listdir(int_dir)
        if f.startswith("iter_intention_") and f.endswith(".txt")
    ], key=lambda x: int(x.split('_')[-1].split('.')[0]))

    with open('/content/seed1.txt', 'r', encoding='utf-8') as f:
        prev_text = f.read().strip()

    records = []
    for i in range(len(generation_files)):
        with open(generation_files[i], 'r', encoding='utf-8') as fg:
            after_text = fg.read().strip()
        with open(intention_files[i], 'r', encoding='utf-8') as fi:
            label = fi.read().strip()
        records.append({
            'before text': prev_text,
            'after text': after_text,
            'label': label,
            'model': model_name
        })
        prev_text = after_text

    df = pd.DataFrame(records)
    os.makedirs(output_folder, exist_ok=True)
    output_path = os.path.join(output_folder, f"{model_name}_output.csv")
    df.to_csv(output_path, index=False)

    shutil.rmtree(temp_dir)
    print(f"Saved CSV to: {output_path}")

process_zip_to_csv("/content/all_gpt-4o_with_data_ref_outputs.zip", "/content/gpt-4o-data", "gpt-4o-data-reference")

# process_zip_to_csv("", "/content/gpt-4o-data", "gpt-4o-data-reference")
process_zip_to_csv("/content/all_llama-sw_with_data_ref_outputs.zip", "/content/gpt-4o-data", "llama-sw-data-reference")
# process_zip_to_csv("/content/all_gpt-4o-outputs.zip", "/content/gpt-4o-data", "gpt-4o")
process_zip_to_csv("/content/all_gpt-o1-mini_with_data_ref_outputs.zip", "/content/gpt-4o-data", "gpt-o1-mini-data-reference")
process_zip_to_csv("/content/all_gpt4-turbo_outputs.zip", "/content/gpt-4o-data", "gpt-4-turbo")
process_zip_to_csv("/content/all_gpt4-turbo_with_data_ref_outputs.zip", "/content/gpt-4o-data", "gpt-4-turbo-data-reference")
process_zip_to_csv("/content/all_gpto1-mini_outputs.zip", "/content/gpt-4o-data", "gpt-o1-mini")

process_zip_to_csv("/content/all_gpt-4o-outputs.zip", "/content/gpt-4o-data", "gpt-4o")

process_zip_to_csv("/content/all_llama_sw_outputs.zip", "/content/gpt-4o-data", "llama-sw")
process_zip_to_csv("/content/all_llama_meta_outputs.zip", "/content/gpt-4o-data", "llama-meta")

def merge_all_model_outputs(folder_path):
    all_dfs = []

    for file in os.listdir(folder_path):
        if file.endswith(".csv") and file != "model_outputs.csv":
            file_path = os.path.join(folder_path, file)
            df = pd.read_csv(file_path)
            df['index_in_file'] = range(len(df))
            all_dfs.append(df)

    if not all_dfs:
        print("No CSV files found to merge.")
        return

    merged_df = pd.concat(all_dfs, ignore_index=True)
    output_path = os.path.join(folder_path, "model_outputs.csv")
    merged_df.to_csv(output_path, index=False)
    print(f"Merged CSV saved to: {output_path}")

merge_all_model_outputs("/content/gpt-4o-data")

def extract_zip(zip_path, extract_dir):
  with zipfile.ZipFile(zip_path, 'r') as zip_ref:
      zip_ref.extractall(extract_dir)

  print("finished：", extract_dir)

zip_path = "/content/all_llama_sw_outputs.zip"
extract_dir = "/content/all_llama_sw_outputs"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("finished：", extract_dir)

zip_path = "/content/all_outputs (1).zip"
extract_dir = "/content/all_gpt_outputs"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("finished：", extract_dir)

zip_path = "/content/all_llama_meta_outputs.zip"
extract_dir = "/content/all_meta_outputs"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("finished：", extract_dir)

zip_path = "/content/all_gpt4-turbo_outputs.zip"
extract_dir = "/content/all_turbo_outputs"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("finished：", extract_dir)

zip_path = "/content/all_gpto1-mini_outputs.zip"
extract_dir = "/content/all_o1_outputs"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("finished：", extract_dir)

zip_path = "/content/all_gpt4-turbo_with_data_ref_outputs.zip"
extract_dir = "/content/all_turbo_wdr_outputs"



extract_zip(zip_path, extract_dir)

zip_path = "/content/all_gpt-o1-mini_with_data_ref_outputs.zip"
extract_dir = "/content/all_o1-mini_wdr_outputs"

extract_zip(zip_path, extract_dir)

zip_path = "/content/all_llama-sw_with_data_ref_outputs.zip"
extract_dir = "/content/all_llama-sw_wdr_outputs"

extract_zip(zip_path, extract_dir)

extract_zip("/content/all_gpt-4o_with_data_ref_outputs.zip", "/content/all_4o_wdr_outputs")

def extract_to_array(base)
for i in range(100):
    file_path = f"{base_path}/iter_intention_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            intention_texts.append(text)
    except FileNotFoundError:
        print(f"Missing file: {file_path}")
        intention_texts.append("")

print(f"Loaded {len(intention_texts)} files.")

intention_texts = []

base_path = "/content/all_gpt_outputs/seed1_openai/intention_openai"

for i in range(100):
    file_path = f"{base_path}/iter_intention_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            intention_texts.append(text)
    except FileNotFoundError:
        print(f"Missing file: {file_path}")
        intention_texts.append("")

print(f"Loaded {len(intention_texts)} files.")

intention_texts_sw = []

base_path = "/content/all_llama_sw_outputs/intention"

for i in range(100):
    file_path = f"{base_path}/iter_intention_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            intention_texts_sw.append(text)
    except FileNotFoundError:
        print(f"Missing file: {file_path}")
        intention_texts_sw.append("")

print(f"Loaded {len(intention_texts_sw)} files.")

intention_texts_meta = []

base_path = "/content/all_meta_outputs/intention"

for i in range(100):
    file_path = f"{base_path}/iter_intention_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            intention_texts_meta.append(text)
    except FileNotFoundError:
        print(f"Missing file: {file_path}")
        intention_texts_meta.append("")

print(f"Loaded {len(intention_texts_meta)} files.")

intention_texts_meta = []

base_path = "/content/all_meta_outputs/intention"

for i in range(100):
    file_path = f"{base_path}/iter_intention_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            intention_texts_meta.append(text)
    except FileNotFoundError:
        print(f"Missing file: {file_path}")
        intention_texts_meta.append("")

print(f"Loaded {len(intention_texts_meta)} files.")

def build_intention_array(base_path):
  intention_texts_meta = []
  for i in range(100):
      file_path = f"{base_path}/iter_intention_{i}.txt"
      try:
          with open(file_path, "r", encoding="utf-8") as f:
              text = f.read().strip()
              intention_texts_meta.append(text)
      except FileNotFoundError:
          print(f"Missing file: {file_path}")
          intention_texts_meta.append("")

  print(f"Loaded {len(intention_texts_meta)} files.")
  return intention_texts_meta

intention_texts_turbo_data = build_intention_array("/content/all_turbo_wdr_outputs/intention_openai")

intention_texts_o1_data = build_intention_array("/content/all_o1-mini_wdr_outputs/intention_openai")

intention_texts_4o_data = build_intention_array("/content/all_4o_wdr_outputs/intention_openai")

intention_texts_o1 = []

base_path = "/content/all_o1_outputs/intention_openai"


intention_texts_o1 = build_intention_array(base_path)
intention_texts_turbo = build_intention_array("/content/all_turbo_outputs/intention_openai")

intention_texts_sw_data = build_intention_array("/content/all_llama-sw_wdr_outputs/intention")

"""# Frequency Analysis"""

from collections import Counter

freq_counter = Counter(intention_texts)

sorted_freq = freq_counter.most_common()

for i, (intent, freq) in enumerate(sorted_freq[:10]):
    print(f"[{i+1}] {freq} times:\n{intent[:100]}{'...' if len(intent) > 100 else ''}\n")

freq_counter = Counter(intention_texts_sw)

sorted_freq = freq_counter.most_common()

for i, (intent, freq) in enumerate(sorted_freq[:10]):
    print(f"[{i+1}] {freq} times:\n{intent[:100]}{'...' if len(intent) > 100 else ''}\n")

"""# Semantic Analysis

## Cosine Similarity between Final Output and Seed
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

with open("/content/all_gpt_outputs/seed1_openai/generation_openai/iter_generation_0.txt", "r", encoding="utf-8") as f:
    text_0 = f.read().strip()

with open("/content/all_gpt_outputs/seed1_openai/generation_openai/iter_generation_99.txt", "r", encoding="utf-8") as f:
    text_99 = f.read().strip()

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform([text_0, text_99])

cos_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]

print(f"Cosine similarity between iter 0 and iter 99: {cos_sim:.4f}")

with open("/content/all_llama_sw_outputs/generation/iter_generation_0.txt", "r", encoding="utf-8") as f:
    text_0 = f.read().strip()

with open("/content/all_llama_sw_outputs/generation/iter_generation_99.txt", "r", encoding="utf-8") as f:
    text_99 = f.read().strip()

tfidf_matrix = vectorizer.fit_transform([text_0, text_99])
cos_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]

print(f"Cosine similarity between iter 0 and iter 99: {cos_sim:.4f}")

with open("/content/all_meta_outputs/generation/iter_generation_0.txt", "r", encoding="utf-8") as f:
    text_0 = f.read().strip()

with open("/content/all_meta_outputs/generation/iter_generation_99.txt", "r", encoding="utf-8") as f:
    text_99 = f.read().strip()

tfidf_matrix = vectorizer.fit_transform([text_0, text_99])
cos_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]

print(f"Cosine similarity between iter 0 and iter 99: {cos_sim:.4f}")

def calculate_lexical_diversity(filepath):
    import re
    with open(filepath, 'r', encoding='utf-8') as f:
        text = f.read().lower()

    tokens = re.findall(r'\b\w+\b', text)

    if not tokens:
        return 0.0

    unique_tokens = set(tokens)
    lexical_diversity = len(unique_tokens) / len(tokens)

    return lexical_diversity

calculate_lexical_diversity("/content/all_gpt_outputs/seed1_openai/generation_openai/iter_generation_99.txt")

calculate_lexical_diversity("/content/all_llama_sw_outputs/generation/iter_generation_99.txt")

calculate_lexical_diversity("/content/all_meta_outputs/generation/iter_generation_99.txt")

"""### Cosine Similarity Analyses
- GPT: 0.5134
- SW: 0.1548
- Meta: 0.5156

### Lexical Diversity
- GPT: 0.3758
- SW: 0.3863
- Meta: 0.3219

## Similarities Differences Changes(with Last Iteration)
"""

generation_texts = []
base_path = "/content/all_llama_sw_outputs/generation"

for i in range(100):
    file_path = f"{base_path}/iter_generation_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            generation_texts.append(text)
    except FileNotFoundError:
        generation_texts.append("")

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(generation_texts)

cos_sim_list = []
for i in range(1, 100):
    sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[i-1])[0][0]
    cos_sim_list.append(sim)

plt.figure(figsize=(12, 5))
plt.plot(range(1, 100), cos_sim_list, marker='o', color='darkred')
plt.title("[SW]Cosine Similarity Between Consecutive Generations (iter 0 → 99)")
plt.xlabel("Iteration (i vs i-1)")
plt.ylabel("Cosine Similarity")
plt.grid(True)
plt.show()

generation_texts = []
base_path = "/content/all_gpt_outputs/seed1_openai/generation_openai"

for i in range(100):
    file_path = f"{base_path}/iter_generation_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            generation_texts.append(text)
    except FileNotFoundError:
        generation_texts.append("")

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(generation_texts)

cos_sim_list = []
for i in range(1, 100):
    sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[i-1])[0][0]
    cos_sim_list.append(sim)

plt.figure(figsize=(12, 5))
plt.plot(range(1, 100), cos_sim_list, marker='o', color='darkred')
plt.title("[OpenAI]Cosine Similarity Between Consecutive Generations (iter 0 → 99)")
plt.xlabel("Iteration (i vs i-1)")
plt.ylabel("Cosine Similarity")
plt.grid(True)
plt.show()

generation_texts = []
base_path = "/content/all_meta_outputs/generation"

for i in range(100):
    file_path = f"{base_path}/iter_generation_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            generation_texts.append(text)
    except FileNotFoundError:
        generation_texts.append("")

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(generation_texts)

cos_sim_list = []
for i in range(1, 100):
    sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[i-1])[0][0]
    cos_sim_list.append(sim)

plt.figure(figsize=(12, 5))
plt.plot(range(1, 100), cos_sim_list, marker='o', color='darkred')
plt.title("[SW]Cosine Similarity Between Consecutive Generations (iter 0 → 99)")
plt.xlabel("Iteration (i vs i-1)")
plt.ylabel("Cosine Similarity")
plt.grid(True)
plt.show()

"""## Cosine Similarities Changes(with Initial Seed)"""

generation_texts = []
base_path = "/content/all_llama_sw_outputs/generation"

for i in range(100):
    file_path = f"{base_path}/iter_generation_{i}.txt"
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            generation_texts.append(text)
    except FileNotFoundError:
        generation_texts.append("")

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(generation_texts)

cos_sim_list = []
for i in range(1, 100):
    sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[0])[0][0]
    cos_sim_list.append(sim)

plt.figure(figsize=(12, 5))
plt.plot(range(1, 100), cos_sim_list, marker='o', color='darkred')
plt.title("[SW]Cosine Similarity Between Consecutive Generations (iter 0 → 99)")
plt.xlabel("Iteration (i vs i-1)")
plt.ylabel("Cosine Similarity")
plt.grid(True)
plt.show()

cos_sim_list = []
for i in range(1, 100):
    sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[0])[0][0]
    cos_sim_list.append(sim)

plt.figure(figsize=(12, 5))
plt.plot(range(1, 100), cos_sim_list, marker='o', color='darkred')
plt.title("[OpenAI]Cosine Similarity Between Final Generations and Seed")
plt.xlabel("Iteration (i vs i-1)")
plt.ylabel("Cosine Similarity")
plt.grid(True)
plt.show()

cos_sim_list = []
for i in range(1, 100):
    sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[0])[0][0]
    cos_sim_list.append(sim)

plt.figure(figsize=(12, 5))
plt.plot(range(1, 100), cos_sim_list, marker='o', color='darkred')
plt.title("[Meta]Cosine Similarity Between Final Generations and Seed")
plt.xlabel("Iteration (i vs i-1)")
plt.ylabel("Cosine Similarity")
plt.grid(True)
plt.show()

"""## Intention Transition Visualization"""

import pandas as pd

def array_to_dataframe(intention_labels):
  transitions = []
  for i in range(len(intention_labels) - 1):
      source = intention_labels[i]
      target = intention_labels[i + 1]
      transitions.append((source, target))

  transition_df = pd.DataFrame(transitions, columns=["source", "target"])
  return transition_df

import pandas as pd
import plotly.graph_objects as go
import plotly.express as px

def visualize_intention_transitions(intention_labels):
    label_order = [
        'Idea Generation', 'Idea Organization', 'Section Planning',
        'Text Production', 'Object Insertion', 'Cross-reference',
        'Citation Integration', 'Macro Insertion', 'Fluency',
        'Coherence', 'Clarity', 'Structural', 'Linguistic Style',
        'Scientific Accuracy', 'Visual Formatting'
    ]

    transitions = [(intention_labels[i], intention_labels[i + 1]) for i in range(len(intention_labels) - 1)]
    transition_df = pd.DataFrame(transitions, columns=["source", "target"])

    trans_counts = transition_df.value_counts().reset_index(name="count")
    trans_counts = trans_counts[
        trans_counts["source"].isin(label_order) & trans_counts["target"].isin(label_order)
    ]
    trans_counts = trans_counts[trans_counts["source"] != trans_counts["target"]].reset_index(drop=True)

    source_cat = pd.Categorical(trans_counts["source"], categories=label_order, ordered=True)
    color_indices = source_cat.codes
    deep_colors = px.colors.qualitative.Plotly
    n_colors = len(deep_colors)
    colorscale = [[i / (n_colors - 1), c] for i, c in enumerate(deep_colors)]

    fig = go.Figure(go.Parcats(
        dimensions=[
            dict(values=trans_counts["source"], label="Source Label"),
            dict(values=trans_counts["target"], label="Target Label")
        ],
        counts=trans_counts["count"],
        line=dict(
            color=color_indices,
            colorscale=colorscale,
            shape='hspline'
        ),
        arrangement='freeform'
    ))

    fig.update_layout(
        title="Intention Label Transitions (Parcats Visualization)",
        height=600,
        margin=dict(l=50, r=50, t=80, b=50)
    )

    fig.show()

import plotly.io as pio
from IPython.display import display

pio.renderers.default = 'colab'

# Label config
label_order = [
    'Idea Generation', 'Idea Organization', 'Section Planning',
    'Text Production', 'Object Insertion', 'Cross-reference',
    'Citation Integration', 'Macro Insertion', 'Fluency',
    'Coherence', 'Clarity', 'Structural', 'Linguistic Style',
    'Scientific Accuracy', 'Visual Formatting'
]

label_to_high = {
    'Idea Generation': 'PLANNING',
    'Idea Organization': 'PLANNING',
    'Section Planning': 'PLANNING',
    'Text Production': 'IMPLEMENTATION',
    'Object Insertion': 'IMPLEMENTATION',
    'Cross-reference': 'IMPLEMENTATION',
    'Citation Integration': 'IMPLEMENTATION',
    'Macro Insertion': 'IMPLEMENTATION',
    'Fluency': 'REVISION',
    'Coherence': 'REVISION',
    'Clarity': 'REVISION',
    'Structural': 'REVISION',
    'Linguistic Style': 'REVISION',
    'Scientific Accuracy': 'REVISION',
    'Visual Formatting': 'REVISION'
}

high_level_groups = {
    'PLANNING': {
        'labels': ['Idea Generation', 'Idea Organization', 'Section Planning'],
        'color': 'rgba(255, 237, 204, 0.4)'
    },
    'IMPLEMENTATION': {
        'labels': ['Text Production', 'Object Insertion', 'Cross-reference',
                 'Citation Integration', 'Macro Insertion'],
        'color': 'rgba(204, 255, 229, 0.4)'
    },
    'REVISION': {
        'labels': ['Fluency', 'Coherence', 'Clarity', 'Structural',
                 'Linguistic Style', 'Scientific Accuracy', 'Visual Formatting'],
        'color': 'rgba(221, 204, 255, 0.4)'
    }
}

def visualize_intention_flow(intention_labels):
    df = pd.DataFrame({"label": intention_labels})
    df = df[df["label"].isin(label_order)].copy()
    df['label'] = pd.Categorical(df['label'], categories=label_order, ordered=True)
    df['high_level'] = df['label'].map(label_to_high)
    df = df.reset_index(drop=True)

    simplified = df.copy()
    simplified['step'] = range(len(simplified))

    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=simplified['step'],
        y=simplified['label'],
        mode='lines+markers',
        line=dict(width=2, color='rgb(31, 119, 180)'),
        marker=dict(size=8, color='rgb(31, 119, 180)'),
        hovertemplate="<b>Step %{x}</b><br>%{y}<extra></extra>"
    ))

    for hl, group in high_level_groups.items():
        indices = [label_order.index(l) for l in group['labels']]
        min_idx, max_idx = min(indices), max(indices)
        y_center = (min_idx + max_idx) / 2

        fig.add_shape(
            type="rect",
            xref="paper",
            yref="y",
            x0=0,
            x1=1,
            y0=min_idx - 0.5,
            y1=max_idx + 0.5,
            fillcolor=group['color'],
            layer="below",
            line_width=0
        )

        fig.add_annotation(
            xref="paper",
            yref="y",
            x=0.98,
            y=y_center,
            text=f"<b>{hl}</b>",
            showarrow=False,
            font=dict(size=14, color='black'),
            bgcolor=group['color'],
            bordercolor='gray',
            borderwidth=1,
            align="left",
            xanchor="left"
        )

    fig.update_layout(
        title="Writing Process Flow of Input Intention Sequence",
        xaxis_title="Writing Steps",
        yaxis_title="Writing Activities",
        yaxis=dict(
            categoryorder='array',
            categoryarray=label_order,
            tickfont=dict(size=12)
        ),
        margin=dict(r=120),
        height=700,
        width=1100,
        hovermode='closest',
        plot_bgcolor='white',
        showlegend=False
    )

    fig.add_shape(
        type="line",
        xref="paper",
        yref="paper",
        x0=1,
        x1=1,
        y0=0,
        y1=1,
        line=dict(color="gray", width=1, dash="dot")
    )

    display(fig)

"""## ChatGPT Intention Transition Visualization"""

visualize_intention_transitions(intention_texts)

visualize_intention_flow(intention_texts)

"""## LLAMA-SW Intention Transition Visualization"""

visualize_intention_transitions(intention_texts_sw)

visualize_intention_flow(intention_texts_sw)

"""## LLAMA-Meta Intention Transition Visualization"""

# intention_texts_meta
visualize_intention_transitions(intention_texts_meta)

visualize_intention_flow(intention_texts_meta)

"""## o1 Intention Transition Visualization"""

visualize_intention_transitions(intention_texts_o1)

visualize_intention_flow(intention_texts_o1)

"""## TURBO Intention Transition Visualization"""

visualize_intention_transitions(intention_texts_turbo)

visualize_intention_flow(intention_texts_turbo)

len(intention_texts_turbo)

"""## Turbo(with data reference) Intention Transition Visualization"""

visualize_intention_transitions(intention_texts_turbo_data)

visualize_intention_flow(intention_texts_turbo_data)

"""## o1 with data reference"""

visualize_intention_transitions(intention_texts_o1_data)

visualize_intention_flow(intention_texts_o1_data)

"""## 4o with data reference"""

visualize_intention_transitions(intention_texts_4o_data)

visualize_intention_flow(intention_texts_4o_data)

"""## llama-SW(with data reference) Intent Transition Visualization"""

visualize_intention_transitions(intention_texts_sw_data)

visualize_intention_flow(intention_texts_sw_data)

