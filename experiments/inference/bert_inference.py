# -*- coding: utf-8 -*-
"""ScholaWrite-BERT-Inferences

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/astrohl/scholawrite-bert-inferences.38bbd977-2a67-43d8-b4fb-88233cf9cbc7.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250610/auto/storage/goog4_request%26X-Goog-Date%3D20250610T182743Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5ef5c0fb4c583b4dc59ca8f7ee0f02836db4a508d53ff32023ecc0cfb50596785cca9325cf490dd366417d04182ba35ed4f24e3dab477f164e2eef2406e720c58496d0aae796db21841c59a45e1dd62f0a3dd2a0b6e8f8ed116fbb9b549321524f9b57ca6a2ec7d446738d4ee72c889b19f5e4f3faaa70c1d2820615a57f5dd3614acb7275bf5ffe89b3a4a31a216b5024fa8570fe40990123e82af2a8c42f7d941edf5beaefe4ccdc97649855cbb238e53cd54e3d540e76ba77be8124dcec8f2506ed7f5cf385a333fb36b0f01f8dae6ca22340d54fad8077d474762daf35861757fdc6b6cf39d78fe10e2c06e24abf2de30de0136e8adfecc1387078b226d6
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

astrohl_scholawrite_path = kagglehub.dataset_download('astrohl/scholawrite')

print('Data source import complete.')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import polars as pls

data1 = pd.read_parquet('/kaggle/input/scholawrite/all_sorted-00000-of-00002.parquet', engine='pyarrow')
data2 = pd.read_parquet('/kaggle/input/scholawrite/all_sorted-00001-of-00002.parquet', engine='pyarrow')

test_data1 = pd.read_parquet('/kaggle/input/scholawrite/test-00000-of-00001.parquet', engine='pyarrow')
test_data2 = pd.read_parquet('/kaggle/input/scholawrite/test_small-00000-of-00001.parquet', engine='pyarrow')
train_data2 = pd.read_parquet('/kaggle/input/scholawrite/train-00000-of-00001.parquet', engine='pyarrow')

test_data1.shape, test_data2.shape, train_data2.shape

!pip install --upgrade bitsandbytes transformers accelerate unsloth

!pip install dotenv

import os
from unsloth import FastLanguageModel
from dotenv import load_dotenv
from huggingface_hub import login
import torch

load_dotenv()
login(os.getenv("KEY"))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from datasets import load_dataset

ds = load_dataset("minnesotanlp/scholawrite")["train"].to_pandas()

le = LabelEncoder()
le.fit(ds["label"])

ALL_LABELS = le.classes_.tolist()
print("Author-defined label order:", ALL_LABELS)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit_transform(train_data2["label"])

ALL_LABELS = le.classes_.tolist()
print("order:", ALL_LABELS)

def testing_loop_with_bert(test_dataset, model1, model2, tokenizer, ALL_LABELS, device="cpu"):
    labels1 = []
    labels2 = []

    for index, row in test_dataset.iterrows():
        before_text = row['before text']
        true_label = row['label']

        text = "<INPUT><BT>" + before_text + "</BT></INPUT>"

        input = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding="max_length"
        )

        input = {k: v.to(device) for k, v in input.items()}

        with torch.no_grad():
            logits1 = model1(**input).logits
            logits2 = model2(**input).logits

            pred1_idx = logits1.argmax(1).item()
            pred2_idx = logits2.argmax(1).item()

        predicted_label1 = ALL_LABELS[pred1_idx]
        predicted_label2 = ALL_LABELS[pred2_idx]

        print(f"[{index+1}/{len(test_dataset)}] GT: {true_label}")
        print(f"  model1 → {predicted_label1}")
        print(f"  model2 → {predicted_label2}")

        labels1.append(predicted_label1)
        labels2.append(predicted_label2)

        # if index == 1:
        #     break

    return labels1, labels2

from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer.add_tokens(["<INPUT>", "</INPUT>", "<BT>", "</BT>", "<PWA>", "</PWA>"])

bert_model = BertForSequenceClassification.from_pretrained(
    'minnesotanlp/scholawrite-bert-classifier',
    num_labels=TOTAL_CLASSES
)

bert_model.resize_token_embeddings(len(tokenizer))

before_text = test_data2['before text'][0]
text = "<INPUT><BT>" + before_text + "</BT></INPUT>"

input = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

with torch.no_grad():
    logits = bert_model(input["input_ids"]).logits
    pred = logits.argmax(1).item()

label = ALL_LABELS[pred]
print("Predicted label:", label)

robert_model = BertForSequenceClassification.from_pretrained('minnesotanlp/scholawrite-roberta-classifier', num_labels=TOTAL_CLASSES)
robert_model.resize_token_embeddings(len(tokenizer))
before_text = test_data2['before text'][0]
text = "<INPUT>" + "<BT>" + before_text + "</BF> " + "</INPUT>"
input = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

with torch.no_grad():
    logits = robert_model(input["input_ids"]).logits
    pred = logits.argmax(1).item()

label = ALL_LABELS[pred]
print("Predicted label:", label)

bert_model.to(device)
robert_model.to(device)
results = testing_loop_with_bert(test_data2, bert_model, robert_model, tokenizer, le.classes_.tolist(), device)

labels1, labels2 = results

df = pd.DataFrame({
    "model1_prediction": labels1,
    "model2_prediction": labels2
})

df.to_csv("/kaggle/working/bert_model_predictions.csv", index=False)

labels1_idx = [ALL_LABELS.index(label) for label in labels1]
labels2_idx = [ALL_LABELS.index(label) for label in labels2]

df = pd.DataFrame({
    "model1_index": labels1_idx,
    "model2_index": labels2_idx,
})


df.to_csv("/kaggle/working/bert_model_predictions_with_index.csv", index=False)

df.head()

test_data2.head()

from sklearn.metrics import accuracy_score

true_label_idx   = [ALL_LABELS.index(label) for label in test_data2['label']]
pred1_label_idx  = [ALL_LABELS.index(label) for label in labels1]
pred2_label_idx  = [ALL_LABELS.index(label) for label in labels2]
acc1 = accuracy_score(true_label_idx, pred1_label_idx)
acc2 = accuracy_score(true_label_idx, pred2_label_idx)

print(f"Model 1 Accuracy: {acc1:.4f}")
print(f"Model 2 Accuracy: {acc2:.4f}")

label_counts = train_data2['label'].value_counts(normalize=True)

label_percent = label_counts * 100
label_percent

