# -*- coding: utf-8 -*-
"""ScholaWrite-LLM-Inferences

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/astrohl/scholawrite-llm-inferences.916b65db-9c41-4f59-a669-63c321fc7ec8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250610/auto/storage/goog4_request%26X-Goog-Date%3D20250610T182829Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc5a2fc75759dd4eb8742e028d44b881bf3c75c556fd6e935ac9d5fd6bc660f2869541ff838261d6ad9bc3125f48f3b5e89414f49637e9e7b210e3d934ba8ade2795b95d1eb5a60211a7d184a4939c308af1d8f6ad71103c323aba27385a38c22b5f0ebd617b6152824d80f5835c5725224a30aa1d9c861c58a99f8c5ef9e263c6c6866c800450468fde420384a930233fe6c6f026ba18f95ff2873bcd1b02a4c858e104f7f1042c2921107af31fc19c014e747f7010f987d55b627576dfcf68ed66d4d8af82d60b8c191e9fc3bca7bcc81c0e221e184c1974c7dc4599150e2726266dc3848b516ae67775309de9e70b33b04caba2bcf349e99d7e07d3dbb695a
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

astrohl_scholawrite_path = kagglehub.dataset_download('astrohl/scholawrite')
astrohl_printed_logs_llama_path = kagglehub.dataset_download('astrohl/printed-logs-llama')

print('Data source import complete.')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import polars as pls

data1 = pd.read_parquet('/kaggle/input/scholawrite/all_sorted-00000-of-00002.parquet', engine='pyarrow')
data2 = pd.read_parquet('/kaggle/input/scholawrite/all_sorted-00001-of-00002.parquet', engine='pyarrow')

test_data1 = pd.read_parquet('/kaggle/input/scholawrite/test-00000-of-00001.parquet', engine='pyarrow')
test_data2 = pd.read_parquet('/kaggle/input/scholawrite/test_small-00000-of-00001.parquet', engine='pyarrow')
train_data2 = pd.read_parquet('/kaggle/input/scholawrite/train-00000-of-00001.parquet', engine='pyarrow')

test_data1.shape, test_data2.shape, train_data2.shape

data1.head()

data1['before text'][0]

!pip install --upgrade bitsandbytes transformers accelerate unsloth

!pip install dotenv

"""# Writing Intention Predictive Error Analysis"""

test_data1.head()

test_data2.head()

import os
from unsloth import FastLanguageModel
from dotenv import load_dotenv
from huggingface_hub import login
import torch

load_dotenv()
login(os.getenv("KEY"))

ALL_LABELS = [
    'Text Production', 'Visual Formatting', 'Clarity', 'Section Planning',
    'Structural', 'Object Insertion', 'Cross-reference', 'Fluency',
    'Idea Generation', 'Idea Organization', 'Citation Integration', 'Coherence',
    'Linguistic Style', 'Scientific Accuracy', 'Macro Insertion'
]

model_name = "minnesotanlp/scholawrite-llama3.1-8b-classifier"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=4096,
    load_in_4bit=True,
    dtype=None,
)
FastLanguageModel.for_inference(model)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def high_class_prompt(before_text, after_text):
    return [
        {
            "role": "user",
            "content": f"""Classify the writing intention of the following text revision. Choose from the following categories:
'IMPLEMENTATION', 'REVISION', 'PLANNING'

Before Text: {before_text};
After Text: {after_text}"""
        }
    ]

def process_label(predicted_label, labels):
    for true_label in labels:
        if true_label in predicted_label:
            return true_label
    print(f"Unrecognized label: {predicted_label}")
    return "Invalid"

HIGH_LEVEL_LABELS = [
    'IMPLEMENTATION', 'REVISION', 'PLANNING'
]

test_data2_high_labels = []
test_data2_labels = []

dropped_test_data2 = test_data2.drop(columns=['label','high-level'])
dropped_test_data2.head()

persona_definition = {
  "Idea Generation": "formulate and record initial thoughts and concepts.",
  "Idea Organization": "select the most useful materials and demarcate those generated ideas in a visually formatted way.",
  "Section Planning": "initially create sections and sub-level structures.",
  "Text Production": "translate your ideas into full languages, either from your language or borrowed sentences from an external source.",
  "Object Insertion": "insert visual claims of your arguments (e.g., figures, tables, equations, footnotes, itemized lists, etc.).",
  "Cross-reference": "link different sections, figures, tables, or other elements within a document through referencing commands.",
  "Citation Integration": "incorporate bibliographic references into a document and systematically link these references using citation commands.",
  "Macro Insertion": "incorporate predefined commands orc packages into a LaTeX document to alter its formatting.",
  "Fluency": "fix grammatical or syntactic errors in the text or LaTeX commands.",
  "Coherence": "logically link (1) any of the two or multiple sentences within the same paragraph; (2) any two subsequent paragraphs; or (3) objects to be consistent as a whole.",
  "Structural": "improve the flow of information by modifying the location of texts and objects.",
  "Clarity": "improve the semantic relationships between texts to be more straightforward and concise.",
  "Linguistic Style": "modify texts with your writing preferences regarding styles and word choices, etc.",
  "Scientific Accuracy": "update or correct scientific evidence (e.g., numbers, equations) for more accurate claims.",
  "Visual Formatting": "modify the stylistic formatting of texts, objects, and citations."
}


def text_gen_prompt(before_text, writing_intention):
    user_prompt = f"""You are a computer science researcher with extensive experience of scholarly writing. Here, you are writing a research paper in natural language processing using LaTeX languages.

Your writing intention is to {persona_definition[writing_intention]}

Below is the paper you have written so far. Please strictly follow the writing intention given above and insert, delete, or revise at appropriate places in the paper given below.

Your writing should relate to the paper given below. Do not generate text other than paper content. Do not describe the changes you are making or your reasoning.

{before_text}"""

    return [
        {"role": "user", "content": user_prompt}
    ]


def class_prompt(before_text):
    usr_prompt= f"""Here are all the possible writing intention labels:

Idea Generation: Formulate and record initial thoughts and concepts.
Idea Organization: Select the most useful materials and demarcate those generated ideas in a visually formatted way.
Section Planning: Initially create sections and sub-level structures.
Text Production: Translate their ideas into full languages, either from the writers' language or borrowed sentences from an external source.
Object Insertion: Insert visual claims of their arguments (e.g., figures, tables, equations, footnotes, itemized lists, etc.).
Cross-reference: Link different sections, figures, tables, or other elements within a document through referencing commands.
Citation Integration: Incorporate bibliographic references into a document and systematically link these references using citation commands.
Macro Insertion: Incorporate predefined commands or packages into a LaTeX document to alter its formatting.
Fluency: Fix grammatical or syntactic errors in the text or LaTeX commands.
Coherence: Logically link (1) any of the two or multiple sentences within the same paragraph; (2) any two subsequent paragraphs; or (3) objects to be consistent as a whole.
Structural: Improve the flow of information by modifying the location of texts and objects.
Clarity: Improve the semantic relationships between texts to be more straightforward and concise.
Linguistic Style: Modify texts with the writer's writing preferences regarding styles and word choices, etc.
Scientific Accuracy: Update or correct scientific evidence (e.g., numbers, equations) for more accurate claims.
Visual Formatting: Modify the stylistic formatting of texts, objects, and citations.

Identify the most likely next writing intention of a graduate researcher when editing the following LaTex paper draft. Your output should only be a label from the list above.

{before_text}"""

    return [
        {"role": "user", "content": usr_prompt}
    ]

# loop
def testing_loop(test_dataset):
    tf_records = []
    test_data2_labels = []
    for index, row in test_dataset.iterrows():
        before_text = row['before text']

        chat = class_prompt(before_text)

        input_ids = tokenizer.apply_chat_template(
            chat,
            max_length=4096,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(device)

        attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=30,
                do_sample=True,
                top_k=50,
                top_p=0.95
            )

        response = tokenizer.batch_decode(outputs)[0]
        response = response.split("<|start_header_id|>assistant<|end_header_id|>")[1].strip()
        response = response.replace("<|eot_id|>", "")

        predicted_detailed_label = process_label(response, ALL_LABELS)
        print("Predicted intention:", predicted_detailed_label, test_data2['label'][index])

        if predicted_detailed_label != test_data2['label'][index]:
            tf_records.append(False)
        else:
            tf_records.append(True)

        test_data2_labels.append(predicted_detailed_label)
        print("Round: ", index+1,"/",test_dataset.shape[0])
    return {tf_records, test_data2_labels}

results = testing_loop(dropped_test_data2)

# use printed results instead for analysis
import re
import csv

ALL_LABELS = [
    'Text Production', 'Visual Formatting', 'Clarity', 'Section Planning',
    'Structural', 'Object Insertion', 'Cross-reference', 'Fluency',
    'Idea Generation', 'Idea Organization', 'Citation Integration', 'Coherence',
    'Linguistic Style', 'Scientific Accuracy', 'Macro Insertion'
]

ALL_LABELS = sorted(ALL_LABELS, key=lambda x: -len(x))

with open("/kaggle/input/printed-logs-llama/printed_results.txt", "r") as f:
    text = f.read()

extracted_labels = []

extracted_labels = []

for line in text.splitlines():
    if line.startswith("Predicted intention:"):
        content = line.replace("Predicted intention:", "").strip()

        label_positions = {}
        for label in ALL_LABELS:
            index = content.find(label)
            if index != -1:
                label_positions[label] = index

        if label_positions:
            selected_label = min(label_positions, key=label_positions.get)
            extracted_labels.append(selected_label)



with open("/kaggle/working/predicted_first_label.csv", "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["first_predicted_label"])
    for label in extracted_labels:
        writer.writerow([label])

extracted_labels

test_data_w_pred = test_data2.copy()
test_data_w_pred['llama predicted label'] = extracted_labels
test_data_w_pred.head()

test_data_w_pred.to_csv("/kaggle/working/small_testing_data_with_llama_pred.csv", index=False)

accuracy = (test_data_w_pred['label'] == test_data_w_pred['llama predicted label']).mean()
print(f"acc: {accuracy:.4f}")

from sklearn.metrics import f1_score


true_labels = test_data_w_pred['label']
pred_labels = test_data_w_pred['llama predicted label']

f1 = f1_score(true_labels, pred_labels, average='weighted')
print(f"Weighted F1-score: {f1:.4f}")



def testing_loop_with_bert(test_dataset, model1, model2, tokenizer, device="cpu"):
    labels1 = []
    labels2 = []

    for index, row in test_dataset.iterrows():
        before_text = row['before text']
        text = "<INPUT><BT>" + before_text + "</BF></INPUT>"

        input = tokenizer(text, return_tensors="pt").to(device)

        with torch.no_grad():
            pred1 = model1(input["input_ids"]).logits.argmax(1).item()
            pred2 = model2(input["input_ids"]).logits.argmax(1).item()

        predicted_detailed_label1 = ALL_LABELS[pred1]
        predicted_detailed_label2 = ALL_LABELS[pred2]

        print("Predicted intention (model1):", predicted_detailed_label1, "True:", test_dataset['label'][index])
        print("Predicted intention (model2):", predicted_detailed_label2)
        print("Round: ", index + 1, "/", test_dataset.shape[0])

        labels1.append(predicted_detailed_label1)
        labels2.append(predicted_detailed_label2)

    return labels1, labels2

"""## BERT Model"""

from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification

load_dotenv()
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
login(token=HUGGINGFACE_TOKEN)

TOTAL_CLASSES = 15

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer.add_tokens("<INPUT>")  # start input
tokenizer.add_tokens("</INPUT>") # end input
tokenizer.add_tokens("<BT>")     # before text
tokenizer.add_tokens("</BT>")    # before text
tokenizer.add_tokens("<PWA>")    # start previous writing action
tokenizer.add_tokens("</PWA>")   # end previous writing action

bert_model = BertForSequenceClassification.from_pretrained('minnesotanlp/scholawrite-bert-classifier', num_labels=TOTAL_CLASSES)

before_text = test_data['before text'][0]
text = "<INPUT>" + "<BT>" + before_text + "</BF> " + "</INPUT>"

input = tokenizer(text, return_tensors="pt")
pred = bert_model(input["input_ids"]).logits.argmax(1)
print("class:", pred)

"""## RoBERT Model"""

robert_model = BertForSequenceClassification.from_pretrained('minnesotanlp/scholawrite-roberta-classifier', num_labels=TOTAL_CLASSES)

before_text = test_data['before text'][0]
text = "<INPUT>" + "<BT>" + before_text + "</BF> " + "</INPUT>"

input = tokenizer(text, return_tensors="pt")
pred = robert_model(input["input_ids"]).logits.argmax(1)
print("class:", pred)

robert_results = testing_loop_with_bert(test_data2, bert_model, robert_model, tokenizer)

