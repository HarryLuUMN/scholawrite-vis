# -*- coding: utf-8 -*-
"""gpt-experiments

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/astrohl/gpt-experiments.8306000a-b460-4853-aa27-20027ca3f094.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250610/auto/storage/goog4_request%26X-Goog-Date%3D20250610T182303Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1689fc39601c8aa6dd313efba0a6d9c1dc171c0281d871a840e7ce4a6785c4bebb8e6c8c82e3845c4c738ea61312d5b9367eb875e560fba8bfd47eee5f815f1ed62ae1f055e4f87f36a54a9b3895e535209994f7e1624e140310c975fd861353d1ca1c121c1aab9baea7b509d3bda492b2a2bc88a82477cc0b610823c1e6f256f75cf7873c01b4efe0441df2d5f4342ca58b578a64dd76cea52d072adeb1a19124c4b1d66ee1f4f25f2c4e87f04517b05228bf1e6494065ba6305c0b0affc6b9f24669df077a8c84530f6af6759eab0d87466c26a8ab44995e144e721941fa224e32f6380f5aab35a8051abc6f9bbbb0d87e7f618100742962a4a0bf0bef0b24
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

astrohl_scholawrite_path = kagglehub.dataset_download('astrohl/scholawrite')
astrohl_scholawrite_seeds_path = kagglehub.dataset_download('astrohl/scholawrite-seeds')

print('Data source import complete.')

import pandas as pd
import re
import os
import numpy as np
from tqdm import tqdm
from openai import OpenAI

persona_definition = {
  "Idea Generation": "formulate and record initial thoughts and concepts.",
  "Idea Organization": "select the most useful materials and demarcate those generated ideas in a visually formatted way.",
  "Section Planning": "initially create sections and sub-level structures.",
  "Text Production": "translate your ideas into full languages, either from your language or borrowed sentences from an external source.",
  "Object Insertion": "insert visual claims of your arguments (e.g., figures, tables, equations, footnotes, itemized lists, etc.).",
  "Cross-reference": "link different sections, figures, tables, or other elements within a document through referencing commands.",
  "Citation Integration": "incorporate bibliographic references into a document and systematically link these references using citation commands.",
  "Macro Insertion": "incorporate predefined commands orc packages into a LaTeX document to alter its formatting.",
  "Fluency": "fix grammatical or syntactic errors in the text or LaTeX commands.",
  "Coherence": "logically link (1) any of the two or multiple sentences within the same paragraph; (2) any two subsequent paragraphs; or (3) objects to be consistent as a whole.",
  "Structural": "improve the flow of information by modifying the location of texts and objects.",
  "Clarity": "improve the semantic relationships between texts to be more straightforward and concise.",
  "Linguistic Style": "modify texts with your writing preferences regarding styles and word choices, etc.",
  "Scientific Accuracy": "update or correct scientific evidence (e.g., numbers, equations) for more accurate claims.",
  "Visual Formatting": "modify the stylistic formatting of texts, objects, and citations."
}

def class_prompt(before_text):
    usr_prompt= f"""Here are all the possible writing intention labels:

Idea Generation: Formulate and record initial thoughts and concepts.
Idea Organization: Select the most useful materials and demarcate those generated ideas in a visually formatted way.
Section Planning: Initially create sections and sub-level structures.
Text Production: Translate their ideas into full languages, either from the writers' language or borrowed sentences from an external source.
Object Insertion: Insert visual claims of their arguments (e.g., figures, tables, equations, footnotes, itemized lists, etc.).
Cross-reference: Link different sections, figures, tables, or other elements within a document through referencing commands.
Citation Integration: Incorporate bibliographic references into a document and systematically link these references using citation commands.
Macro Insertion: Incorporate predefined commands or packages into a LaTeX document to alter its formatting.
Fluency: Fix grammatical or syntactic errors in the text or LaTeX commands.
Coherence: Logically link (1) any of the two or multiple sentences within the same paragraph; (2) any two subsequent paragraphs; or (3) objects to be consistent as a whole.
Structural: Improve the flow of information by modifying the location of texts and objects.
Clarity: Improve the semantic relationships between texts to be more straightforward and concise.
Linguistic Style: Modify texts with the writer's writing preferences regarding styles and word choices, etc.
Scientific Accuracy: Update or correct scientific evidence (e.g., numbers, equations) for more accurate claims.
Visual Formatting: Modify the stylistic formatting of texts, objects, and citations.

Identify the most likely next writing intention of a graduate researcher when writing the following LaTex paper draft. Your output should only be a label from the list above.

{before_text}"""

    return [
        {"role": "user", "content": usr_prompt}
    ]


def text_gen_prompt(before_text, writing_intention):

    user_prompt = f"""You are a computer science researcher with extensive experience in scholarly writing. Here, you are writing a research paper in natural language processing using LaTeX.

You currently want to {persona_definition[writing_intention]}

Below is the paper you have written so far. Given the paper information below and the corresponding scholarly writing intention, please revise or add to the text to fulfill this writing intention.

You may insert, delete, or revise text at appropriate places in the given paper.

Please provide a complete output. Do not generate text that is nonsensical or unrelated to the given paper information.

Your response should limited to 2000 word tokens

{before_text}"""

    return [
        {"role": "user", "content": user_prompt}
    ]

def clean_text(text):
    text = re.sub(r"<del>.*?<\/del>", "", text, flags=re.DOTALL)

    text = re.sub(r"<same>(.*?)<\/same>", r"\1", text, flags=re.DOTALL)

    text = re.sub(r"<add>(.*?)<\/add>", r"\1", text, flags=re.DOTALL)

    tags_to_remove = ["<add>", "</add>", "<del>", "</del>", "<same>", "</same>"]
    for tag in tags_to_remove:
        text = text.replace(tag, "")

    return text

def save_raw_output(output, writing_intention, i):
  generation_raw_dir = '/kaggle/working/'
  with open(f"{generation_raw_dir}/iter_generation_{i}.txt", "w") as file:
    file.write(output)

  with open(f"{intention_raw_dir}/iter_intention_{i}.txt", "w") as file:
    file.write(writing_intention)

ALL_LABELS = [
    'Text Production', 'Visual Formatting', 'Clarity', 'Section Planning',
    'Structural', 'Object Insertion', 'Cross-reference', 'Fluency',
    'Idea Generation', 'Idea Organization', 'Citation Integration', 'Coherence',
    'Linguistic Style', 'Scientific Accuracy', 'Macro Insertion'
]

def process_label(predicted_label):
    all_labels = ['Text Production', 'Visual Formatting', 'Clarity', 'Section Planning',
 'Structural', 'Object Insertion', 'Cross-reference', 'Fluency',
 'Idea Generation', 'Idea Organization', 'Citation Integration', 'Coherence',
 'Linguistic Style', 'Scientific Accuracy', 'Macro Insertion']


    if predicted_label not in all_labels:
        found = 0
        for true_label in all_labels:
            if true_label in predicted_label:
                predicted_label = true_label
                found = 1
                break

        # If the output from gpt didn't contain any expeceted label
        if found != 1:
            predicted_label = "Text Production"

    return predicted_label

client = OpenAI(api_key="OPENAI_API")
# OPENAI_MODEL = "gpt-4-turbo"
OPENAI_MODEL = "gpt-4o"

def predict_intention_openai(text):
    messages = class_prompt(text)
    response = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=messages
    )
    raw_label = response.choices[0].message.content.strip()
    return process_label(raw_label)


def writing_inference_openai(before_text, writing_intention):
    messages = text_gen_prompt(before_text, writing_intention)
    response = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=messages
    )
    return response.choices[0].message.content.strip()


def aggregate_iterative_writing_openai():
    prev_writing = load_seed(path_to_seed)
    i = 0
    pbar = tqdm(total=100)

    while i < 100:
        # Step 1: Predict next intention
        writing_intention = predict_intention_openai(prev_writing)

        # Step 2: Generate revised text
        output = writing_inference_openai(prev_writing, writing_intention)

        # Step 3: Clean output
        cleaned_output = clean_text(output)

        # Step 4: Save outputs
        with open(f"{generation_dir}/iter_generation_{i}.txt", "w") as f_gen:
            f_gen.write(cleaned_output)

        with open(f"{intention_dir}/iter_intention_{i}.txt", "w") as f_intent:
            f_intent.write(writing_intention)

        save_raw_output(output, writing_intention, i)
        print("finished iteration ", i, writing_intention)

        # Step 5: Update state
        prev_writing = cleaned_output
        i += 1
        pbar.update(1)

def setup_openai(seed_name):
    global generation_dir, intention_dir, generation_raw_dir, intention_raw_dir, path_to_seed

    output_dir = f"/kaggle/working/{seed_name}_openai"

    generation_dir = f"{output_dir}/generation_openai"
    intention_dir = f"{output_dir}/intention_openai"
    generation_raw_dir = f"{output_dir}/generation_raw_openai"
    intention_raw_dir = f"{output_dir}/intention_raw_openai"

    os.makedirs(generation_dir, exist_ok=True)
    os.makedirs(intention_dir, exist_ok=True)
    os.makedirs(generation_raw_dir, exist_ok=True)
    os.makedirs(intention_raw_dir, exist_ok=True)

    path_to_seed = f"/kaggle/input/scholawrite-seeds/{seed_name}.txt"


def main_openai():
    for seed_name in ["seed1"]:
        print(f"Working on {seed_name} with OpenAI API")
        setup_openai(seed_name)
        aggregate_iterative_writing_openai()
        print("-" * 100)

def load_seed(fname):
  with open(fname, 'r') as file:
    return file.read()

main_openai()
import shutil
shutil.make_archive("/kaggle/working/all_gpt4o_outputs", 'zip', "/kaggle/working/seed1_openai")

"""## GPT with Data Insertion
### Data Processing
"""

data1 = pd.read_parquet('/kaggle/input/scholawrite/all_sorted-00000-of-00002.parquet', engine='pyarrow')
data2 = pd.read_parquet('/kaggle/input/scholawrite/all_sorted-00001-of-00002.parquet', engine='pyarrow')
data1.shape, data2.shape

df = pd.concat([data1, data2], axis=0, ignore_index=True)

import difflib

def char_level_diff(before, after):
    matcher = difflib.SequenceMatcher(None, before, after)
    del_text_parts = []
    add_text_parts = []

    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'replace' or tag == 'delete':
            del_text_parts.append(before[i1:i2])
        if tag == 'replace' or tag == 'insert':
            add_text_parts.append(after[j1:j2])

    return ''.join(del_text_parts).strip(), ''.join(add_text_parts).strip()

df['del_text'], df['add_text'] = zip(*df.apply(lambda row: char_level_diff(row['before text'], row['after text']), axis=1))

def paragraph_diff(before_text, after_text):
    before_paras = [p.strip() for p in before_text.split('\n') if p.strip()]
    after_paras = [p.strip() for p in after_text.split('\n') if p.strip()]

    matcher = difflib.SequenceMatcher(None, before_paras, after_paras)

    changed_before = []
    changed_after = []

    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag != 'equal':
            changed_before.extend(before_paras[i1:i2])
            changed_after.extend(after_paras[j1:j2])

    return '\n'.join(changed_before).strip(), '\n'.join(changed_after).strip()

df.head()

df['before_paragraph'], df['after_paragraph'] = zip(*df.apply(
    lambda row: paragraph_diff(row['before text'], row['after text']),
    axis=1
))

df.head()

label_counts = df['label'].value_counts()
print(label_counts)

df['before_len'] = df['before_paragraph'].str.len()
df['after_len'] = df['after_paragraph'].str.len()

print("Before Paragraph Length Stats:")
print(df['before_len'].describe())

print("\nAfter Paragraph Length Stats:")
print(df['after_len'].describe())

def balanced_sample(df, n_per_label):
    sampled = (
        df[['label', 'before_paragraph', 'after_paragraph']].groupby('label')
        .apply(lambda g: g.sample(n=min(n_per_label, len(g)), random_state=42))
        .reset_index(drop=True)
    )
    return sampled

n_per_label = 20
df_filtered = df[
    (df['before_len'] > 50) & (df['after_len'] > 50) &
    (df['before_len'] < 800) & (df['after_len'] < 800)
]

final_prompt_df = balanced_sample(df_filtered, n_per_label)

print(final_prompt_df['label'].value_counts())

final_prompt_df

def class_prompt_with_data_ref(before_text, examples):
    example_block = "\n\n".join([
        f"""### Example
Before:
{ex['before_paragraph']}

After:
{ex['after_paragraph']}

Label:
{ex['label']}""" for ex in examples
    ])

    usr_prompt = f"""Here are all the possible writing intention labels:

{chr(10).join([f"{label}: {desc}" for label, desc in persona_definition.items()])}

Below are examples of revisions and their corresponding writing intentions:

{example_block}

Now, identify the most likely next writing intention of a graduate researcher when writing the following LaTex paper draft. Your output should only be a label from the list above.

{before_text}"""

    return [{"role": "user", "content": usr_prompt}]

def text_gen_prompt_with_data_ref(before_text, writing_intention, examples):
    example_block = "\n\n".join([
        f"""### Example
Writing Intention: {ex['label']}

Before:
{ex['before_paragraph']}

After:
{ex['after_paragraph']}""" for ex in examples
    ])

    user_prompt = f"""You are a computer science researcher with extensive experience in scholarly writing. Here, you are writing a research paper in natural language processing using LaTeX.

You currently want to {persona_definition[writing_intention]}

Below are examples of how other researchers revised their drafts to fulfill various scholarly writing intentions:

{example_block}

Below is the paper you have written so far. Given the paper information below and the corresponding scholarly writing intention, please revise or add to the text to fulfill this writing intention.

You may insert, delete, or revise text at appropriate places in the given paper.

Please provide a complete output. Do not generate text that is nonsensical or unrelated to the given paper information.

Your response should be limited to 2000 word tokens.

{before_text}"""

    return [{"role": "user", "content": user_prompt}]

examples = final_prompt_df[['before_paragraph', 'after_paragraph', 'label']].sample(n=10, random_state=42).to_dict(orient='records')

def predict_intention_openai_with_data_ref(text):
    messages = class_prompt_with_data_ref(text, examples)
    response = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=messages
    )
    raw_label = response.choices[0].message.content.strip()
    return process_label(raw_label)


def writing_inference_openai_with_data_ref(before_text, writing_intention):
    messages = text_gen_prompt_with_data_ref(before_text, writing_intention, examples)
    response = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=messages
    )
    return response.choices[0].message.content.strip()

def aggregate_iterative_writing_openai_with_data_ref():
    prev_writing = load_seed(path_to_seed)
    i = 0
    pbar = tqdm(total=100)

    while i < 100:
        # Step 1: Predict next intention
        writing_intention = predict_intention_openai_with_data_ref(prev_writing)

        # Step 2: Generate revised text
        output = writing_inference_openai_with_data_ref(prev_writing, writing_intention)

        # Step 3: Clean output
        cleaned_output = clean_text(output)

        # Step 4: Save outputs
        with open(f"{generation_dir}/iter_generation_{i}.txt", "w") as f_gen:
            f_gen.write(cleaned_output)

        with open(f"{intention_dir}/iter_intention_{i}.txt", "w") as f_intent:
            f_intent.write(writing_intention)

        save_raw_output(output, writing_intention, i)
        print("finished iteration ", i, writing_intention)

        # Step 5: Update state
        prev_writing = cleaned_output
        i += 1
        pbar.update(1)


def main_openai_with_data_ref():
    for seed_name in ["seed1"]:
        print(f"Working on {seed_name} with OpenAI API")
        setup_openai(seed_name)
        aggregate_iterative_writing_openai_with_data_ref()
        print("-" * 100)

main_openai_with_data_ref()
import shutil
shutil.make_archive("/kaggle/working/all_gpt-4o_with_data_ref_outputs", 'zip', "/kaggle/working/seed1_openai")

